{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Qxa2s-eRpU_e",
   "metadata": {
    "id": "Qxa2s-eRpU_e"
   },
   "source": [
    "# Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "F09kK6YLpU_k",
   "metadata": {
    "id": "F09kK6YLpU_k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 07:52:12.277292: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-22 07:52:12.310662: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6K_YSDWpU_l",
   "metadata": {
    "id": "p6K_YSDWpU_l"
   },
   "source": [
    "Preprocessing data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "droXxAoXpU_m",
   "metadata": {
    "id": "droXxAoXpU_m"
   },
   "outputs": [],
   "source": [
    "# ==== PREPROCESSING ====\n",
    "def preprocess(image_path, mask_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img_rgb = img_rgb.astype(np.float32) / 255.0\n",
    "\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE))\n",
    "    mask = (mask > 127).astype(np.float32)\n",
    "\n",
    "    return img_rgb, mask[..., np.newaxis]\n",
    "\n",
    "def load_dataset(image_dir, mask_dir):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    X, Y = [], []\n",
    "\n",
    "    for fname in image_files:\n",
    "        image_path = os.path.join(image_dir, fname)\n",
    "        mask_name = fname.replace('.jpg', '_HGE_Seg.jpg')\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            img, mask = preprocess(image_path, mask_path)\n",
    "            X.append(img)\n",
    "            Y.append(mask)\n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nnYVTtabpU_m",
   "metadata": {
    "id": "nnYVTtabpU_m"
   },
   "source": [
    "## Defining Constants and Useful Functions##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aqt9fupopU_n",
   "metadata": {
    "id": "aqt9fupopU_n"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 # resize all of our images to 128 like with the benchmark u-net, suraj: changed to 224 from 128, starting point should be size of image\n",
    "NUM_ACTIONS = 9\n",
    "HISTORY_SIZE = 10 #\n",
    "STATE_DIM = 4096 + NUM_ACTIONS * HISTORY_SIZE\n",
    "#Changes max steps from 20 to 200\n",
    "MAX_STEPS = 200\n",
    "EXPERIENCE_SAMPLE_SIZE = 20\n",
    "MAX_EXPERIENCE_SIZE = 1000\n",
    "GAMMA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "VSwOEGDTpU_n",
   "metadata": {
    "id": "VSwOEGDTpU_n"
   },
   "outputs": [],
   "source": [
    "@tf.function(reduce_retracing=True)\n",
    "def extract_feature_tf(image, history, feature_extractor):\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image.set_shape([IMG_SIZE, IMG_SIZE, 3])\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "    history_onehot = tf.one_hot(history, depth=NUM_ACTIONS, dtype=tf.float32)\n",
    "    history_flat = tf.reshape(history_onehot, [-1])\n",
    "\n",
    "    features = feature_extractor(image)\n",
    "    features_flat = tf.reshape(features, [-1])\n",
    "\n",
    "    return tf.concat([features_flat, history_flat], axis=0)\n",
    "\n",
    "\n",
    "def compute_q(feature, model):\n",
    "    return model.predict(feature, verbose=0).flatten()\n",
    "\n",
    "def compute_mask(action, box):\n",
    "  # changing from 0.1 to 0.2(alpha)\n",
    "    delta_w = 0.2 * (box[2] - box[0])\n",
    "    delta_h = 0.2 * (box[3] - box[1])\n",
    "    x1, y1, x2, y2 = box\n",
    "\n",
    "    if action == 0:  # move right\n",
    "        x1 += delta_w\n",
    "        x2 += delta_w\n",
    "    elif action == 1:  # move left\n",
    "        x1 -= delta_w\n",
    "        x2 -= delta_w\n",
    "    elif action == 2:  # move up\n",
    "        y1 -= delta_h\n",
    "        y2 -= delta_h\n",
    "    elif action == 3:  # move down\n",
    "        y1 += delta_h\n",
    "        y2 += delta_h\n",
    "    elif action == 4:  # zoom in (proportional)\n",
    "        x1 += delta_w\n",
    "        x2 -= delta_w\n",
    "        y1 += delta_h\n",
    "        y2 -= delta_h\n",
    "    elif action == 5:  # zoom out (proportional)\n",
    "        x1 -= delta_w\n",
    "        x2 += delta_w\n",
    "        y1 -= delta_h\n",
    "        y2 += delta_h\n",
    "    elif action == 6:  # vertical zoom in (squish height)\n",
    "        y1 += delta_h\n",
    "        y2 -= delta_h\n",
    "    elif action == 7:  # horizontal zoom in (squish width)\n",
    "        x1 += delta_w\n",
    "        x2 -= delta_w\n",
    "\n",
    "    # clip box to valid image bounds\n",
    "    x1 = max(0, min(IMG_SIZE, x1))\n",
    "    x2 = max(0, min(IMG_SIZE, x2))\n",
    "    y1 = max(0, min(IMG_SIZE, y1))\n",
    "    y2 = max(0, min(IMG_SIZE, y2))\n",
    "\n",
    "    # enforce correct box ordering\n",
    "    x1, x2 = sorted([x1, x2])\n",
    "    y1, y2 = sorted([y1, y2])\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    inter = max(0, xB - xA) * max(0, yB - yA)\n",
    "    areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    return inter / float(areaA + areaB - inter + 1e-6)\n",
    "\n",
    "def crop_image(img, box):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    cropped = img[y1:y2, x1:x2]\n",
    "    return cv2.resize(cropped, (IMG_SIZE, IMG_SIZE)) if cropped.size else np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6TFS-nopU_o",
   "metadata": {
    "id": "o6TFS-nopU_o"
   },
   "source": [
    "## Reward Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KTrbo8r7pU_o",
   "metadata": {
    "id": "KTrbo8r7pU_o"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is a section on rewards. Currently we define 3 different functions:\n",
    "binary -\n",
    "IoU difference -\n",
    "IoU penalty area -\n",
    "\n",
    "each of the these take in the inputs action, gt, box, and end.\n",
    "\"\"\"\n",
    "\n",
    "def reward_binary(action, gt, box, end=False, timed_out=False):\n",
    "    new_iou = compute_iou(compute_mask(action, box), gt)\n",
    "    old_iou = compute_iou(box, gt)\n",
    "\n",
    "    if end:\n",
    "        if timed_out:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 3 if new_iou > 0.1 else -3\n",
    "    else:\n",
    "        reward = 1 if new_iou > old_iou else -1\n",
    "    return reward\n",
    "\n",
    "def reward_iou_diff(action, gt, box, end=False, timed_out=False):\n",
    "    new_iou = compute_iou(compute_mask(action, box), gt)\n",
    "    old_iou = compute_iou(box, gt)\n",
    "\n",
    "    if end:\n",
    "        if timed_out:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 3 if new_iou > 0.6 else -3\n",
    "    else:\n",
    "        \n",
    "        reward = new_iou - old_iou\n",
    "    return reward\n",
    "\n",
    "def reward_iou_area_penalty(action, gt, box, end=False, timed_out=False):\n",
    "    new_box = compute_mask(action, box)\n",
    "    new_iou = compute_iou(new_box, gt)\n",
    "    area = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])\n",
    "    norm_area = area / (IMG_SIZE * IMG_SIZE)\n",
    "\n",
    "    if end:\n",
    "        if timed_out:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = 3 if new_iou > 0.6 else -3\n",
    "    else:\n",
    "        reward = new_iou - 0.1 * norm_area\n",
    "    return reward\n",
    "\n",
    "def reward_focus_area(action, gt, box, end=False, timed_out=False):\n",
    "    new_box = compute_mask(action, box)\n",
    "    iou = compute_iou(new_box, gt)\n",
    "\n",
    "    # Dimensions of current box\n",
    "    box_w = new_box[2] - new_box[0]\n",
    "    box_h = new_box[3] - new_box[1]\n",
    "    full_area = IMG_SIZE * IMG_SIZE\n",
    "\n",
    "    # Prevent division by zero or nonsense\n",
    "    if box_w <= 0 or box_h <= 0:\n",
    "        return -1\n",
    "\n",
    "    box_area = box_w * box_h\n",
    "\n",
    "    # If box covers entire image (initial state), give 0 reward\n",
    "    if box_area >= full_area * 0.99:  # Added small tolerance for floating point\n",
    "        return 0.0\n",
    "\n",
    "    # How many times this box could fit in the full image (using float division)\n",
    "    fit_multiplier = full_area / box_area\n",
    "\n",
    "    reward = iou * fit_multiplier\n",
    "\n",
    "    if end:\n",
    "        if timed_out:\n",
    "            return 0\n",
    "        else:\n",
    "            return 3 if iou > 0.6 else -3\n",
    "\n",
    "    return reward\n",
    "\n",
    "def reward_iou_diff_area(action, gt, box, end=False, timed_out=False):\n",
    "    # Compute the new bounding box after applying the action.\n",
    "    new_box = compute_mask(action, box)\n",
    "    \n",
    "    # Compute IoU of new box and old box with the ground truth.\n",
    "    new_iou = compute_iou(new_box, gt)\n",
    "    old_iou = compute_iou(box, gt)\n",
    "    \n",
    "    # Hyperparameters for tuning:\n",
    "    scale_factor = 15.0         # Multiply incremental IoU change to boost the reward signal.\n",
    "    area_penalty_weight = 0.05   # Weight for penalizing large bounding boxes.\n",
    "    \n",
    "    # Calculate the normalized area (new_box area relative to full image area).\n",
    "    box_area = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])\n",
    "    full_area = IMG_SIZE * IMG_SIZE\n",
    "    norm_area = box_area / full_area\n",
    "    \n",
    "    # For non-terminal steps, reward is the scaled IoU improvement minus area penalty.\n",
    "    incremental_reward = scale_factor * (new_iou - old_iou) - area_penalty_weight * norm_area\n",
    "\n",
    "    if end:\n",
    "        if timed_out:\n",
    "            reward = 0\n",
    "        else:\n",
    "            # At terminal steps, give a clear positive or negative reward.\n",
    "            # Here, if new IoU exceeds the threshold (0.6), reward is 3; otherwise, -3.\n",
    "            reward = 3 if new_iou > 0.3 else -3\n",
    "    else:\n",
    "        reward = incremental_reward\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ANv6IeyXpU_p",
   "metadata": {
    "id": "ANv6IeyXpU_p"
   },
   "outputs": [],
   "source": [
    "# ==== MODEL + AGENT ====\n",
    "from tensorflow.keras.layers import Dropout\n",
    "def create_q_model():\n",
    "    model = Sequential([\n",
    "      Dense(1024, activation='relu', input_shape=(STATE_DIM,)),\n",
    "      Dropout(0.5),\n",
    "      Dense(1024, activation='relu'),\n",
    "      Dropout(0.5),\n",
    "      Dense(NUM_ACTIONS),\n",
    "    ])\n",
    "    #added accuracy\n",
    "    #model.compile(optimizer='adam', loss='mse', metrics = ['accuracy'])\n",
    "    model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9), loss='mse',  metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, model, epsilon=1.0):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = 0.99\n",
    "        self.memory = []  \n",
    "\n",
    "    def remember(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > 10000:  # optional memory cap\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(NUM_ACTIONS)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def train(self, batch_size=32):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Use tf.stack instead of np.array to ensure proper tensor shape\n",
    "        states = tf.stack(states)\n",
    "        next_states = tf.stack(next_states)\n",
    "\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "        target_qs = q_values.copy()\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            target = rewards[i]\n",
    "            if not dones[i]:\n",
    "                target += self.gamma * np.max(next_q_values[i])\n",
    "            target_qs[i][actions[i]] = target\n",
    "\n",
    "        self.model.train_on_batch(states, target_qs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "X-riHdBdpU_q",
   "metadata": {
    "id": "X-riHdBdpU_q"
   },
   "outputs": [],
   "source": [
    "def train_dqn(X_train, Y_train, vgg16_model, reward_fn, epochs=10, epsilon=1.0):\n",
    "    feature_extractor = tf.keras.Model(inputs=vgg16_model.input, outputs=vgg16_model.layers[20].output)\n",
    "    model = create_q_model()  # should return a compiled model with input shape (STATE_DIM,)\n",
    "    agent = QAgent(model, epsilon=epsilon)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i in range(len(X_train)):\n",
    "            img, mask = X_train[i], Y_train[i]\n",
    "            ys, xs = np.where(mask.squeeze() > 0)\n",
    "            if len(xs) == 0 or len(ys) == 0:\n",
    "                continue\n",
    "\n",
    "            gt_box = [np.min(xs), np.min(ys), np.max(xs), np.max(ys)]\n",
    "            history = [-1] * HISTORY_SIZE\n",
    "            current_box = [0, 0, IMG_SIZE, IMG_SIZE]\n",
    "\n",
    "            img_tensor = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "            history_tensor = tf.convert_to_tensor(history, dtype=tf.int32)\n",
    "            state = extract_feature_tf(img_tensor, history_tensor, feature_extractor)\n",
    "\n",
    "            done, step = False, 0\n",
    "            while not done:\n",
    "                action = agent.select_action(state.numpy())\n",
    "                next_box = current_box if action == 8 else compute_mask(action, current_box)\n",
    "                end = action == 8 or step >= MAX_STEPS\n",
    "                timed_out = step >= MAX_STEPS and action != 8\n",
    "                reward = reward_fn(action, gt_box, current_box, end=end, timed_out=timed_out)\n",
    "\n",
    "                if step % 100 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Image {i}, Step {step}, Action {action}, Reward {reward:.4f}\")\n",
    "\n",
    "\n",
    "                history = history[1:] + [action]\n",
    "                next_crop = crop_image(img, next_box).astype(np.float32)\n",
    "                next_crop_tensor = tf.convert_to_tensor(next_crop, dtype=tf.float32)\n",
    "                history_tensor = tf.convert_to_tensor(history, dtype=tf.int32)\n",
    "                next_state = extract_feature_tf(next_crop_tensor, history_tensor, feature_extractor)\n",
    "\n",
    "                agent.remember((state, action, reward, next_state, end))\n",
    "                state = next_state\n",
    "                current_box = next_box\n",
    "                step += 1\n",
    "                done = end\n",
    "\n",
    "        agent.train(batch_size=32)\n",
    "        EPS_START, EPS_END, EPS_DECAY_EPOCHS = 1.0, 0.1, 5\n",
    "        if epoch < EPS_DECAY_EPOCHS:\n",
    "            agent.epsilon = EPS_START - (EPS_START - EPS_END) * (epoch / (EPS_DECAY_EPOCHS - 1))\n",
    "        else:\n",
    "            agent.epsilon = EPS_END\n",
    "        #agent.epsilon = max(agent.epsilon * 0.95, 0.05)\n",
    "        #agent.epsilon = max(agent.epsilon * 0.98, 0.05)\n",
    "\n",
    "    return agent.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6AltKoGypU_q",
   "metadata": {
    "id": "6AltKoGypU_q"
   },
   "outputs": [],
   "source": [
    "# ==== EVALUATION ====\n",
    "def evaluate_model(model, X_test, Y_test, vgg16_model):\n",
    "    feature_extractor = tf.keras.Model(\n",
    "        inputs=vgg16_model.input,\n",
    "        outputs=vgg16_model.layers[20].output\n",
    "    )\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def extract_feature_tf(image, history):\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        image.set_shape([224, 224, 3])\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "        history_onehot = tf.one_hot(history, depth=NUM_ACTIONS, dtype=tf.float32)\n",
    "        history_flat = tf.reshape(history_onehot, [-1])\n",
    "\n",
    "        features = feature_extractor(image)\n",
    "        features_flat = tf.reshape(features, [-1])\n",
    "\n",
    "        return tf.concat([features_flat, history_flat], axis=0)\n",
    "\n",
    "    ious = []\n",
    "    for i in range(len(X_test)):\n",
    "        img = X_test[i].astype(np.float32)\n",
    "        mask = Y_test[i]\n",
    "\n",
    "        ys, xs = np.where(mask.squeeze() > 0)\n",
    "        if len(xs) == 0 or len(ys) == 0:\n",
    "            continue\n",
    "\n",
    "        gt_box = [np.min(xs), np.min(ys), np.max(xs), np.max(ys)]\n",
    "        history = [-1] * HISTORY_SIZE\n",
    "        current_box = [0, 0, IMG_SIZE, IMG_SIZE]\n",
    "\n",
    "        img_tensor = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "        history_tensor = tf.convert_to_tensor(history, dtype=tf.int32)\n",
    "        state = extract_feature_tf(img_tensor, history_tensor)\n",
    "\n",
    "        done, step = False, 0\n",
    "        while not done:\n",
    "            state_batched = tf.expand_dims(state, axis=0)\n",
    "            q_values = model.predict(state_batched, verbose=0)[0]\n",
    "            action = np.argmax(q_values)\n",
    "\n",
    "            next_box = current_box if action == 8 else compute_mask(action, current_box)\n",
    "            done = action == 8 or step >= MAX_STEPS\n",
    "            history = history[1:] + [action]\n",
    "\n",
    "            next_crop = crop_image(img, next_box).astype(np.float32)\n",
    "            next_crop_tensor = tf.convert_to_tensor(next_crop, dtype=tf.float32)\n",
    "            history_tensor = tf.convert_to_tensor(history, dtype=tf.int32)\n",
    "            state = extract_feature_tf(next_crop_tensor, history_tensor)\n",
    "\n",
    "            current_box = next_box\n",
    "            step += 1\n",
    "\n",
    "        iou = compute_iou(current_box, gt_box)\n",
    "        ious.append(iou)\n",
    "        print(f\"Image {i}, Final IoU: {iou:.4f}\")\n",
    "        print(\"Q-values:\", q_values)\n",
    "\n",
    "    avg_iou = np.mean(ious)\n",
    "    print(f\"\\nAverage IoU on test set: {avg_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1xkqArxpU_r",
   "metadata": {
    "id": "r1xkqArxpU_r"
   },
   "source": [
    "Similar code to before evaluating binary overlap or not (i.e. we get a reward for having overlap with the ground truth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b82783d6-680a-4482-9f64-8cc7f70d9f0f",
   "metadata": {
    "id": "d9Wxir9ypU_r"
   },
   "outputs": [],
   "source": [
    "def binary_overlap_score(model, image_dir, mask_dir, img_size=128):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for fname in image_files:\n",
    "        img_path = os.path.join(image_dir, fname)\n",
    "        mask_name = fname.replace('.jpg', '_HGE_Seg.jpg')\n",
    "        mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "        if not os.path.exists(mask_path):\n",
    "            continue  # skip unmatched files\n",
    "\n",
    "        # Preprocess input and ground truth\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, (img_size, img_size))\n",
    "        mask_bin = (mask > 127).astype(np.uint8)\n",
    "\n",
    "        # Predict\n",
    "        pred = model.predict(np.expand_dims(img_rgb, axis=0))[0].squeeze().round().astype(np.uint8)\n",
    "\n",
    "        # Check if there's any overlap\n",
    "        overlap = np.logical_and(pred, mask_bin).any()\n",
    "\n",
    "        total += 1\n",
    "        if overlap:\n",
    "            correct += 1\n",
    "\n",
    "    print(f\"Binary overlap score: {correct}/{total}\")\n",
    "    return f\"{correct}/{total}\", Fraction(correct, total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806faa00-c2cc-4e7a-8245-354a64168ef4",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i5jFbvZcpU_t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "i5jFbvZcpU_t",
    "outputId": "226529ee-5331-46bd-d482-ac956cd14a72"
   },
   "outputs": [],
   "source": [
    "#print(model.predict(state, verbose=0)[0])\n",
    "# Loading models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "trained_model = load_model(\"dqn_hemorrhage_model_binary_aug.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "otoAjLCXpU_u",
   "metadata": {
    "id": "otoAjLCXpU_u"
   },
   "outputs": [],
   "source": [
    "def visualize_agent_prediction(model, vgg16_model, image_dir, mask_dir, img_size=224):\n",
    "    import os\n",
    "    import random\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    sample_file = random.choice(image_files)\n",
    "\n",
    "    img_path = os.path.join(image_dir, sample_file)\n",
    "    mask_path = os.path.join(mask_dir, sample_file.replace('.jpg', '_HGE_Seg.jpg'))\n",
    "\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"No mask found for {sample_file}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Load and normalize image\n",
    "    img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img_gray, (img_size, img_size))\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2RGB).astype(np.float32) / 255.0\n",
    "\n",
    "    # Load and process mask\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask_resized = cv2.resize(mask, (img_size, img_size))\n",
    "    mask_bin = (mask_resized > 127).astype(np.uint8)\n",
    "\n",
    "    # Feature extractor\n",
    "    feature_extractor = tf.keras.Model(inputs=vgg16_model.input, outputs=vgg16_model.layers[20].output)\n",
    "\n",
    "    @tf.function\n",
    "    def extract_feature_tf(image, history):\n",
    "        image = tf.image.resize(image, [224, 224])\n",
    "        image.set_shape([224, 224, 3])\n",
    "        image = tf.expand_dims(image, axis=0)\n",
    "\n",
    "        history_onehot = tf.one_hot(history, depth=NUM_ACTIONS, dtype=tf.float32)\n",
    "        history_flat = tf.reshape(history_onehot, [-1])\n",
    "\n",
    "        features = feature_extractor(image)\n",
    "        features_flat = tf.reshape(features, [-1])\n",
    "        return tf.concat([features_flat, history_flat], axis=0)\n",
    "\n",
    "    # Inference loop\n",
    "    history = [-1] * HISTORY_SIZE\n",
    "    current_box = [0, 0, img_size, img_size]\n",
    "    trajectory = [current_box.copy()]\n",
    "    img_tensor = tf.convert_to_tensor(img_rgb, dtype=tf.float32)\n",
    "    history_tensor = tf.convert_to_tensor(history, dtype=tf.int32)\n",
    "    state = extract_feature_tf(img_tensor, history_tensor)\n",
    "\n",
    "    done, step = False, 0\n",
    "    while not done:\n",
    "        action = np.argmax(model.predict(np.expand_dims(state.numpy(), axis=0), verbose=0)[0])\n",
    "        next_box = current_box if action == 8 else compute_mask(action, current_box)\n",
    "        done = action == 8 or step >= MAX_STEPS\n",
    "        history = history[1:] + [action]\n",
    "        next_crop = crop_image(img_rgb, next_box).astype(np.float32)\n",
    "        next_crop_tensor = tf.convert_to_tensor(next_crop, dtype=tf.float32)\n",
    "        history_tensor = tf.convert_to_tensor(history, dtype=tf.int32)\n",
    "        state = extract_feature_tf(next_crop_tensor, history_tensor)\n",
    "        current_box = next_box\n",
    "        trajectory.append(current_box.copy())\n",
    "        step += 1\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    axs[0].imshow(img_rgb)\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    overlay = img_rgb.copy()\n",
    "    overlay[mask_bin == 1] = [1.0, 0.0, 0.0]\n",
    "    axs[1].imshow(overlay)\n",
    "    axs[1].set_title('Ground Truth Mask Overlay')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(img_rgb)\n",
    "    for box in trajectory[:-1]:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        axs[2].add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                           linewidth=1, edgecolor='blue', facecolor='none', alpha=0.3))\n",
    "    x1, y1, x2, y2 = map(int, trajectory[-1])\n",
    "    axs[2].add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                       linewidth=2, edgecolor='lime', facecolor='none', linestyle='--'))\n",
    "    axs[2].set_title('RL Trajectory (Blue) + Final Box (Green)')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4362d65a-8930-486e-a837-ab7a520580b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "X, Y = load_dataset('hemorrhage_CT/images', 'hemorrhage_CT/masks')\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "augmenter = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    zoom_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.9, 1.1],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Example to generate augmented images\n",
    "augmented_images = []\n",
    "augmented_masks = []\n",
    "\n",
    "for img, mask in zip(X_train, Y_train):\n",
    "    img = img.reshape((1,) + img.shape)  # Add batch dim\n",
    "    mask = mask.reshape((1,) + mask.shape)\n",
    "\n",
    "    img_gen = augmenter.flow(img, batch_size=1)\n",
    "    mask_gen = augmenter.flow(mask, batch_size=1)\n",
    "\n",
    "    for _ in range(2):  # Add 2 augmented versions per image\n",
    "        augmented_images.append(next(img_gen)[0])\n",
    "        augmented_masks.append(next(mask_gen)[0])\n",
    "\n",
    "# Combine with original\n",
    "X_train_aug = np.concatenate([X_train, np.array(augmented_images)])\n",
    "Y_train_aug = np.concatenate([Y_train, np.array(augmented_masks)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587611d-018d-44c8-b6d7-f2db0789c65d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "te8f2YJvpU_v",
    "outputId": "d386ceaf-d6be-4e8f-c5b1-234536fcda15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/ds340/students/surajg/.conda/envs/my_conda_env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1, Image 0, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 1, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 2, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 3, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 4, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 5, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 6, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 7, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 8, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 9, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 10, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 11, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 12, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 13, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 14, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 15, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 16, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 17, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 18, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 19, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 20, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 21, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 22, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 23, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 24, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 25, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 26, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 27, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 28, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 29, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 30, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 31, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 32, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 33, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 34, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 35, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 36, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 37, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 38, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 39, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 40, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 41, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 42, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 43, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 44, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 45, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 46, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 47, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 48, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 49, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 50, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 51, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 52, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 53, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 54, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 55, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 56, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 57, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 58, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 59, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 60, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 61, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 62, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 63, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 64, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 65, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 66, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 67, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 68, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 69, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 70, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 71, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 72, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 73, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 74, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 75, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 76, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 77, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 78, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 79, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 80, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 81, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 82, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 83, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 84, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 85, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 86, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 87, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 88, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 89, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 90, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 91, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 92, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 93, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 94, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 95, Step 0, Action 1, Reward -1.0000\n",
      "Epoch 1, Image 96, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 97, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 98, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 99, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 100, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 101, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 102, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 103, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 104, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 105, Step 0, Action 2, Reward -1.0000\n",
      "Epoch 1, Image 106, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 107, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 108, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 109, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 110, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 111, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 112, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 113, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 114, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 115, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 116, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 117, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 118, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 119, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 120, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 121, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 122, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 123, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 124, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 125, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 126, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 127, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 128, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 129, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 130, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 131, Step 0, Action 4, Reward -1.0000\n",
      "Epoch 1, Image 132, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 133, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 134, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 135, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 136, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 137, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 138, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 139, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 140, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 141, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 142, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 143, Step 0, Action 0, Reward -1.0000\n",
      "Epoch 1, Image 144, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 145, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 146, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 147, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 148, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 149, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 150, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 151, Step 0, Action 3, Reward -1.0000\n",
      "Epoch 1, Image 152, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 153, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 154, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 155, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 156, Step 0, Action 8, Reward 3.0000\n",
      "Epoch 1, Image 157, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 158, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 159, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 160, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 161, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 162, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 163, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 164, Step 0, Action 8, Reward 3.0000\n",
      "Epoch 1, Image 165, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 166, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 167, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 168, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 169, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 170, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 171, Step 0, Action 3, Reward -1.0000\n",
      "Epoch 1, Image 172, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 173, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 174, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 175, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 176, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 177, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 178, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 179, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 180, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 181, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 182, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 183, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 184, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 185, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 186, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 187, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 188, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 189, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 190, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 191, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 192, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 193, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 194, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 195, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 196, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 197, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 198, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 199, Step 0, Action 6, Reward -1.0000\n",
      "Epoch 1, Image 200, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 201, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 202, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 203, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 204, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 205, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 206, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 207, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 208, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 209, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 210, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 211, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 212, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 213, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 214, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 215, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 216, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 217, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 218, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 219, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 220, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 221, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 222, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 223, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 224, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 225, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 226, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 227, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 228, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 229, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 230, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 231, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 232, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 233, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 234, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 235, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 236, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 237, Step 0, Action 4, Reward -1.0000\n",
      "Epoch 1, Image 238, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 239, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 240, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 241, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 242, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 243, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 244, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 245, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 246, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 247, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 248, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 249, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 250, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 251, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 252, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 253, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 254, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 257, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 263, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 268, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 269, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 270, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 272, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 274, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 275, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 277, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 279, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 286, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 289, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 290, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 291, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 293, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 297, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 298, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 299, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 303, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 304, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 306, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 308, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 309, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 310, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 311, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 312, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 313, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 314, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 315, Step 0, Action 1, Reward -1.0000\n",
      "Epoch 1, Image 321, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 324, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 325, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 326, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 327, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 328, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 335, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 336, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 337, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 338, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 340, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 341, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 342, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 343, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 344, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 345, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 347, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 348, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 349, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 352, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 353, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 354, Step 0, Action 8, Reward 3.0000\n",
      "Epoch 1, Image 355, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 356, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 358, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 367, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 370, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 373, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 375, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 376, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 379, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 382, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 383, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 387, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 388, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 390, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 394, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 404, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 416, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 418, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 419, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 420, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 421, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 422, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 423, Step 0, Action 8, Reward 3.0000\n",
      "Epoch 1, Image 428, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 429, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 432, Step 0, Action 1, Reward -1.0000\n",
      "Epoch 1, Image 433, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 435, Step 0, Action 2, Reward -1.0000\n",
      "Epoch 1, Image 440, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 442, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 444, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 445, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 447, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 450, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 451, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 456, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 457, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 461, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 463, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 464, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 466, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 467, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 470, Step 0, Action 0, Reward -1.0000\n",
      "Epoch 1, Image 471, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 472, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 473, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 474, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 479, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 480, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 482, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 486, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 487, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 488, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 491, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 493, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 495, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 496, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 497, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 498, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 499, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 502, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 503, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 504, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 505, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 507, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 509, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 510, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 511, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 514, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 517, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 518, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 519, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 522, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 523, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 527, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 531, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 532, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 533, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 536, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 538, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 541, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 542, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 543, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 544, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 545, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 548, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 549, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 550, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 552, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 554, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 556, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 557, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 559, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 563, Step 0, Action 4, Reward -1.0000\n",
      "Epoch 1, Image 564, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 566, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 567, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 568, Step 0, Action 4, Reward -1.0000\n",
      "Epoch 1, Image 575, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 576, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 578, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 582, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 583, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 584, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 587, Step 0, Action 8, Reward 3.0000\n",
      "Epoch 1, Image 590, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 591, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 597, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 598, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 599, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 600, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 603, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 608, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 609, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 610, Step 0, Action 6, Reward -1.0000\n",
      "Epoch 1, Image 615, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 617, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 620, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 621, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 622, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 625, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 627, Step 0, Action 7, Reward 1.0000\n",
      "Epoch 1, Image 628, Step 0, Action 1, Reward -1.0000\n",
      "Epoch 1, Image 630, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 634, Step 0, Action 3, Reward -1.0000\n",
      "Epoch 1, Image 637, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 643, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 644, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 646, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 649, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 650, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 651, Step 0, Action 3, Reward 1.0000\n",
      "Epoch 1, Image 652, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 653, Step 0, Action 4, Reward -1.0000\n",
      "Epoch 1, Image 654, Step 0, Action 2, Reward 1.0000\n",
      "Epoch 1, Image 656, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 659, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 660, Step 0, Action 4, Reward 1.0000\n",
      "Epoch 1, Image 661, Step 0, Action 8, Reward -3.0000\n",
      "Epoch 1, Image 662, Step 0, Action 0, Reward -1.0000\n",
      "Epoch 1, Image 663, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 666, Step 0, Action 0, Reward 1.0000\n",
      "Epoch 1, Image 669, Step 0, Action 5, Reward -1.0000\n",
      "Epoch 1, Image 674, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 675, Step 0, Action 1, Reward 1.0000\n",
      "Epoch 1, Image 676, Step 0, Action 6, Reward 1.0000\n",
      "Epoch 1, Image 678, Step 0, Action 7, Reward 1.0000\n"
     ]
    }
   ],
   "source": [
    "#X, Y = load_dataset('hemorrhage_CT/images', 'hemorrhage_CT/masks')\n",
    "#X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "vgg16.trainable = False\n",
    "same_model = train_dqn(X_train_aug, Y_train_aug, vgg16, reward_fn = reward_binary, epochs=10, epsilon=1.0)\n",
    "same_model.save(\"dqn_hemorrhage_model_binary_false.h5\")\n",
    "#trained_model = train_dqn(X_train_aug, Y_train_aug, vgg16, reward_fn = reward_iou_diff_area, epochs=15, epsilon=1.0)\n",
    "#trained_model.save(\"dqn_hemorrhage_model_diff_area_aug.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x8KpWVR4of4L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x8KpWVR4of4L",
    "outputId": "a0f4f594-3d0f-4d29-dcb1-8e87f0d11973"
   },
   "outputs": [],
   "source": [
    "evaluate_model(trained_model, X_temp, Y_temp, vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uy0jke3OaJik",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "Uy0jke3OaJik",
    "outputId": "f9c395e0-3312-41ff-b60b-227062f61ab4"
   },
   "outputs": [],
   "source": [
    "visualize_agent_prediction(trained_model, vgg16, 'hemorrhage_CT/images', 'hemorrhage_CT/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RjkLku4kpU_w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "RjkLku4kpU_w",
    "outputId": "7bf1e7db-4575-4cff-8bac-65491e335273"
   },
   "outputs": [],
   "source": [
    "def full_image_iou_score(image_dir, mask_dir, img_size=128):\n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    sample_file = random.choice(image_files)\n",
    "\n",
    "    # Paths\n",
    "    img_path = os.path.join(image_dir, sample_file)\n",
    "    mask_path = os.path.join(mask_dir, sample_file.replace('.jpg', '_HGE_Seg.jpg'))\n",
    "\n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"Mask not found for {sample_file}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Load and preprocess\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.resize(mask, (img_size, img_size))\n",
    "    mask_bin = (mask > 127).astype(np.uint8)\n",
    "\n",
    "    ys, xs = np.where(mask_bin > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        print(f\"No tumor in mask for {sample_file}.\")\n",
    "        return\n",
    "\n",
    "    # Define ground truth box\n",
    "    gt_box = [np.min(xs), np.min(ys), np.max(xs), np.max(ys)]\n",
    "\n",
    "    # Define prediction as full image\n",
    "    pred_box = [0, 0, img_size, img_size]\n",
    "\n",
    "    # Compute IoU\n",
    "    iou = compute_iou(pred_box, gt_box)\n",
    "    print(f\"Sample: {sample_file}\")\n",
    "    print(f\"Ground Truth Box: {gt_box}\")\n",
    "    print(f\"Predicted Full Image Box: {pred_box}\")\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "    return iou\n",
    "\n",
    "full_image_iou_score('hemorrhage_CT/images', 'hemorrhage_CT/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hV92K1fvpU_x",
   "metadata": {
    "id": "hV92K1fvpU_x"
   },
   "outputs": [],
   "source": [
    "vals = []\n",
    "for _ in range(20):\n",
    "    vals.append(full_image_iou_score('hemorrhage_CT/images', 'hemorrhage_CT/masks'))\n",
    "print(\"average overall: \", np.mean(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nt7DIDawpU_x",
   "metadata": {
    "id": "Nt7DIDawpU_x"
   },
   "outputs": [],
   "source": [
    "# model_binary = tf.keras.models.load_model('dqn_mini_model.keras')\n",
    "visualize_agent_prediction(model_binary, vgg16, 'mini_dataset/train/images', 'mini_dataset/train/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h3MinEotpU_y",
   "metadata": {
    "id": "h3MinEotpU_y"
   },
   "outputs": [],
   "source": [
    "def visualize_pre_cnn_input(image_path, mask_path):\n",
    "    # --- Load original image and mask ---\n",
    "    original = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if original is None or mask is None:\n",
    "        print(\"Missing file.\")\n",
    "        return\n",
    "\n",
    "    # Resize for CNN\n",
    "    resized_image = cv2.resize(original, (224, 224))\n",
    "    resized_mask = cv2.resize(mask, (224, 224))\n",
    "\n",
    "    # Normalize + convert to RGB for consistency\n",
    "    img_rgb = cv2.cvtColor(original, cv2.COLOR_GRAY2RGB).astype(np.float32) / 255.0\n",
    "    mask_bin = (mask > 127).astype(np.uint8)\n",
    "    resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_GRAY2RGB).astype(np.float32) / 255.0\n",
    "    resized_mask_bin = (resized_mask > 127).astype(np.uint8)\n",
    "\n",
    "    # Create overlays\n",
    "    overlay_original = img_rgb.copy()\n",
    "    overlay_original[mask_bin == 1] = [1.0, 0.0, 0.0]  # red\n",
    "\n",
    "    overlay_resized = resized_rgb.copy()\n",
    "    overlay_resized[resized_mask_bin == 1] = [1.0, 0.0, 0.0]  # red\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    axs[0].imshow(original, cmap='gray')\n",
    "    axs[0].set_title('Original Image')\n",
    "    axs[1].imshow(overlay_original)\n",
    "    axs[1].set_title('Original + Mask Overlay')\n",
    "    axs[2].imshow(resized_image, cmap='gray')\n",
    "    axs[2].set_title('Resized for CNN (224224)')\n",
    "    axs[3].imshow(overlay_resized)\n",
    "    axs[3].set_title('Resized + Mask Overlay')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIdXpKCtpU_z",
   "metadata": {
    "id": "TIdXpKCtpU_z"
   },
   "outputs": [],
   "source": [
    "image_path = 'mini_dataset/test/images/051_30.jpg'\n",
    "mask_path = 'mini_dataset/test/masks/051_30_HGE_Seg.jpg'\n",
    "visualize_pre_cnn_input(image_path, mask_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
