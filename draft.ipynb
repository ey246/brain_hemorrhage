{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1a761c3",
   "metadata": {},
   "source": [
    "# Draft (please ignore, it only does classification not segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7973a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f60978a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_fill_mask(mask):\n",
    "    # Ensure mask is a NumPy array\n",
    "    if isinstance(mask, torch.Tensor):\n",
    "        mask = mask.numpy()\n",
    "\n",
    "    mask = mask.astype(np.uint8)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    return mask\n",
    "\n",
    "def apply_windowing(dcm_data, window_center, window_width):\n",
    "    \"\"\"\n",
    "    Apply windowing to a DICOM image.\n",
    "    \n",
    "    :param dcm_data: pydicom Dataset\n",
    "    :param window_center: Center of the window\n",
    "    :param window_width: Width of the window\n",
    "    :return: Windowed image as a numpy array\n",
    "    \"\"\"\n",
    "    intercept = dcm_data.RescaleIntercept if 'RescaleIntercept' in dcm_data else 0\n",
    "    slope = dcm_data.RescaleSlope if 'RescaleSlope' in dcm_data else 1\n",
    "    image = dcm_data.pixel_array * slope + intercept\n",
    "\n",
    "    min_intensity = (window_center - window_width / 2)\n",
    "    max_intensity = (window_center + window_width / 2)\n",
    "\n",
    "    windowed_image = np.clip(image, min_intensity, max_intensity)\n",
    "    windowed_image = ((windowed_image - min_intensity) / (max_intensity - min_intensity)) * 255.0\n",
    "    return windowed_image.astype(np.uint8)\n",
    "\n",
    "class HemorrhagicDataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_file, transform=None, train=True, split_ratio=0.7):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.diagnosis = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Gather all patient directories\n",
    "        self.all_patients = sorted(os.listdir(os.path.join(self.root_dir, \"Patients_CT\")))\n",
    "        \n",
    "        # Split the patient directories into train and validation sets\n",
    "        split_index = int(len(self.all_patients) * split_ratio)\n",
    "        if train:\n",
    "            self.patient_set = self.all_patients[:split_index]\n",
    "        else:\n",
    "            self.patient_set = self.all_patients[split_index:]\n",
    "        \n",
    "        self.slices = self._gather_slices()\n",
    "\n",
    "    def _gather_slices(self):\n",
    "        slices = []\n",
    "        patients_dir = os.path.join(self.root_dir, \"Patients_CT\")\n",
    "        for patient_number in os.listdir(patients_dir):\n",
    "            patient_dir = os.path.join(patients_dir, patient_number, \"brain\")\n",
    "            if os.path.exists(patient_dir):\n",
    "                for file_name in os.listdir(patient_dir):\n",
    "                    if file_name.endswith(\".jpg\") and \"_HGE_Seg\" not in file_name:\n",
    "                        slice_number = file_name.split('.')[0]\n",
    "                        slices.append((patient_dir, patient_number, slice_number))\n",
    "        return slices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_dir, patient_number, slice_number = self.slices[idx]\n",
    "        image_path = os.path.join(patient_dir, f\"{slice_number}.jpg\")\n",
    "        mask_path = os.path.join(patient_dir, f\"{slice_number}_HGE_Seg.jpg\")\n",
    "\n",
    "        # Load the image and mask\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "        if os.path.exists(mask_path):\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "        else:\n",
    "            mask = Image.new(\"L\", image.size)\n",
    "\n",
    "        diag_row = self.diagnosis[(self.diagnosis['PatientNumber'] == int(patient_number))\n",
    "                                  & (self.diagnosis['SliceNumber'] == int(slice_number))]\n",
    "        label = \"hemorrhagic\" if not diag_row.empty and diag_row.iloc[0]['No_Hemorrhage'] == 0 else \"normal\"\n",
    "\n",
    "        # mask = (mask != 0).astype(np.float32)\n",
    "        \n",
    "        # Convert images and masks to PyTorch tensors\n",
    "        image = transforms.ToTensor()(image)\n",
    "        mask = transforms.ToTensor()(mask)\n",
    "\n",
    "        \n",
    "        sample = {'image': image, 'mask': mask, 'label': label, 'original_type': 'image'}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "            sample['mask'] = self.transform(sample['mask'])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e90c0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemo_root_dir = \"computed-tomography-images-for-intracranial-hemorrhage-detection-and-segmentation-1.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "285736e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(hemo_root_dir, \"hemorrhage_diagnosis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b567364",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "46bc62c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(hemo_root_dir, \"hemorrhage_diagnosis.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e90c0b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HemorrhagicDataset(root_dir=hemo_root_dir, csv_file=csv_file, transform=transform, train=True)\n",
    "val_dataset = HemorrhagicDataset(root_dir=hemo_root_dir, csv_file=csv_file, transform=transform, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53652bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler, ConcatDataset\n",
    "\n",
    "def calculate_class_weights(dataset):\n",
    "    # Get labels for each sample in the dataset\n",
    "    labels = [sample['label'] for sample in dataset]\n",
    "    \n",
    "    # Count the frequency of each class\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Calculate weights for each class\n",
    "    class_weights = 1. / counts\n",
    "    \n",
    "    # Calculate weights for each sample\n",
    "    sample_weights = torch.tensor([class_weights[unique_labels == label][0] for label in labels], dtype=torch.float)\n",
    "\n",
    "    return sample_weights\n",
    "\n",
    "sample_weights = calculate_class_weights(train_dataset)\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoader with sampler\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, sampler=sampler, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a077de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def get_wide_resnet50_2(num_classes=4, weight_path=None):\n",
    "    # Load the Wide-ResNet-50-2 model\n",
    "    model = models.wide_resnet50_2(weights=None)  \n",
    "    \n",
    "    # Modify the first layer to accept grayscale input\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.bn1 = nn.BatchNorm2d(64)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    \n",
    "    # Load custom weights\n",
    "    if weight_path:\n",
    "        model.load_state_dict(torch.load(weight_path, map_location='cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weights_path = \"last_best.pth\"\n",
    "model = get_wide_resnet50_2(num_classes=4, weight_path=weights_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "def static_one_hot_encode(label, num_classes=1):\n",
    "    class_to_index = {\n",
    "        \"acute_ischemic\": 0,\n",
    "        \"hyperacute_ischemic\": 1,\n",
    "        \"hemorrhagic\": 2,\n",
    "        \"normal\": 3\n",
    "    }\n",
    "    \n",
    "    index = class_to_index[label]\n",
    "    one_hot_vector = torch.zeros(num_classes)\n",
    "    one_hot_vector[index] = 1\n",
    "    return one_hot_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
